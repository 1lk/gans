{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math, itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf_data/train-images-idx3-ubyte.gz\n",
      "Extracting tf_data/train-labels-idx1-ubyte.gz\n",
      "Extracting tf_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting tf_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# No rescaling needed. MNIST data is in range [0, 1]\n",
    "mnist = input_data.read_data_sets(\"tf_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Neural Network\n",
    "class Discriminator(object):\n",
    "    def __init__(self):\n",
    "        \"\"\" Init the model with hyper-parameters etc. \"\"\"\n",
    "        # Layer 1 Variables\n",
    "        self.weights1 = tf.Variable(tf.zeros([784, 1024]), name='weights1')\n",
    "        self.biases1 = tf.Variable(tf.zeros([1024]), name='biases1')\n",
    "        # Layer 2 Variables\n",
    "        self.weights2 = tf.Variable(tf.zeros([1024, 512]), name='weights2')\n",
    "        self.biases2 = tf.Variable(tf.zeros([512]), name='biases2')\n",
    "        # Layer 3 Variables\n",
    "        self.weights3 = tf.Variable(tf.zeros([512, 256]), name='weights3')\n",
    "        self.biases3 = tf.Variable(tf.zeros([256]), name='biases3')\n",
    "        # Out Layer Variables\n",
    "        self.weights4 = tf.Variable(tf.zeros([256, 1]), name='weights4')\n",
    "        self.biases4 = tf.Variable(tf.zeros([1]), name='biases4')\n",
    "        # Store Variables in list\n",
    "        self.var_list = [self.weights1, self.biases1, self.weights2, self.biases2, \n",
    "                           self.weights3, self.biases3, self.weights4, self.biases4]\n",
    "            \n",
    "    def inference(self, x):\n",
    "        \"\"\" This is the forward calculation from x to y \"\"\"\n",
    "        x = tf.nn.dropout(tf.nn.leaky_relu(tf.matmul(x, self.weights1) + self.biases1), .3)\n",
    "        x = tf.nn.dropout(tf.nn.leaky_relu(tf.matmul(x, self.weights2) + self.biases2), .3)\n",
    "        x = tf.nn.dropout(tf.nn.leaky_relu(tf.matmul(x, self.weights3) + self.biases3), .3)\n",
    "        x = tf.nn.sigmoid(tf.matmul(x, self.weights4) + self.biases4)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, inference_real, inference_fake):\n",
    "        return -tf.reduce_mean(tf.log(inference_real) + tf.log(1. - inference_fake), name='D_loss')\n",
    "        \n",
    "    def optimize(self, loss):\n",
    "        return tf.train.AdamOptimizer().minimize(loss, name='D_optimizer', var_list=self.var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geneator Neural Network\n",
    "class Generator(object):\n",
    "    def __init__(self):\n",
    "        self._init_variables()\n",
    "            \n",
    "    def _init_variables(self):\n",
    "        with tf.variable_scope(\"G\"):\n",
    "            \"\"\" Init the model with hyper-parameters etc. \"\"\"\n",
    "            # Layer 1 Variables\n",
    "            self.weights1 = tf.Variable(tf.zeros([100, 256]))\n",
    "            self.biases1 = tf.Variable(tf.zeros([256]))\n",
    "            # Layer 2 Variables\n",
    "            self.weights2 = tf.Variable(tf.zeros([256, 512]))\n",
    "            self.biases2 = tf.Variable(tf.zeros([512]))\n",
    "            # Layer 3 Variables\n",
    "            self.weights3 = tf.Variable(tf.zeros([512, 1024]))\n",
    "            self.biases3 = tf.Variable(tf.zeros([1024]))\n",
    "            # Out Layer Variables\n",
    "            self.weights4 = tf.Variable(tf.zeros([1024, 784]))\n",
    "            self.biases4 = tf.Variable(tf.zeros([784]))\n",
    "            # Store Variables in list\n",
    "            self.var_list = [self.weights1, self.biases1, self.weights2, self.biases2, \n",
    "                               self.weights3, self.biases3, self.weights4, self.biases4]\n",
    "            \n",
    "    def inference(self, x):\n",
    "        \"\"\" This is the forward calculation from x to y \"\"\"\n",
    "        x = tf.nn.leaky_relu(tf.matmul(x, self.weights1) + self.biases1)\n",
    "        x = tf.nn.leaky_relu(tf.matmul(x, self.weights2) + self.biases2)\n",
    "        x = tf.nn.leaky_relu(tf.matmul(x, self.weights3) + self.biases3)\n",
    "        x = tf.nn.tanh(tf.matmul(x, self.weights4) + self.biases4)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, inference, discriminator):\n",
    "        G_fake_images_prob = discriminator.inference(inference)\n",
    "        return -tf.reduce_mean(tf.log(G_fake_images_prob), name='G_loss')\n",
    "        \n",
    "    def optimize(self, loss):\n",
    "        # Add a scalar summary for the snapshot loss.\n",
    "        tf.summary.scalar('G_loss', loss)\n",
    "        # Create Adam optimizer\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        # Use optimizer to apply gradients\n",
    "        train_op = optimizer.minimize(loss, name='G_optimizer', var_list=self.var_list)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(n_rows, n_cols):\n",
    "    return np.random.normal(size=(n_rows, n_cols ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PIXELS = 28*28\n",
    "NOISE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Placeholders\n",
    "D_images_real = tf.placeholder(tf.float32, shape=(None, IMAGE_PIXELS))\n",
    "D_images_fake = tf.placeholder(tf.float32, shape=(None, IMAGE_PIXELS))\n",
    "# Initialize D\n",
    "discriminator = Discriminator()\n",
    "# Build a Graph that computes D\n",
    "D_inference_real = discriminator.inference(D_images_real)\n",
    "D_inference_fake = discriminator.inference(D_images_fake)\n",
    "# Add to D's graph the Ops for loss calculation.\n",
    "D_loss = discriminator.loss(D_inference_real, D_inference_fake)\n",
    "# Add the Op to optimize D's variables\n",
    "D_optim = discriminator.optimize(D_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Placeholders\n",
    "G_noise = tf.placeholder(tf.float32, shape=(None, NOISE_SIZE))\n",
    "# Initialize G.\n",
    "generator = Generator()\n",
    "# Build a Graph that computes G\n",
    "G_inference = generator.inference(G_noise)\n",
    "# Add to the Graph the Ops for loss calculation.\n",
    "G_loss = generator.loss(G_inference, discriminator)\n",
    "# Add the Op to optimize G's variables\n",
    "G_optim = generator.optimize(G_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create figure for plotting\n",
    "size_figure_grid = int(math.sqrt(16))\n",
    "fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(6, 6))\n",
    "for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "    ax[i,j].get_xaxis().set_visible(False)\n",
    "    ax[i,j].get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "# Start interactive session\n",
    "session = tf.InteractiveSession()\n",
    "# Init Variables\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Iterate through epochs\n",
    "for i in range(NUM_EPOCHS):\n",
    "\n",
    "    # Train Discriminator\n",
    "    real_images = mnist.train.next_batch(batch_size=BATCH_SIZE)[0]\n",
    "    fake_images = session.run(G_inference, feed_dict={G_noise: noise(BATCH_SIZE,NOISE_SIZE)})\n",
    "    feed_dict = {D_images_real: real_images, D_images_fake: fake_images}\n",
    "    _, D_loss_i = session.run([D_optim, D_loss], feed_dict=feed_dict)\n",
    "    \n",
    "    # Train Generator\n",
    "    feed_dict = {G_noise: noise(BATCH_SIZE,NOISE_SIZE)}\n",
    "    _, G_loss_i, G_inference_i = session.run([G_optim, G_loss, G_inference], feed_dict=feed_dict)\n",
    "    \n",
    "    if (i) % 100 == 0:\n",
    "        display.clear_output(True)\n",
    "        for k in range(16):\n",
    "            i = k//4\n",
    "            j = k%4\n",
    "            ax[i,j].cla()\n",
    "            ax[i,j].imshow(G_inference_i[k,:].reshape(28, 28), cmap='Greys')\n",
    "        display.display(plt.gcf())\n",
    "        print(D_loss_i, G_loss_i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
